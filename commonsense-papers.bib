@inproceedings{speer2017conceptnet,
  title={ConceptNet 5.5: an open multilingual graph of general knowledge},
  author={Speer, Robyn and Chin, Joshua and Havasi, Catherine},
  booktitle={Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence},
  pages={4444--4451},
  year={2017}
}

@article{Al-Khatib2020,
abstract = {{\textless}p{\textgreater}This paper studies the end-to-end construction of an argumentation knowledge graph that is intended to support argument synthesis, argumentative question answering, or fake news detection, among others. The study is motivated by the proven effectiveness of knowledge graphs for interpretable and controllable text generation and exploratory search. Original in our work is that we propose a model of the knowledge encapsulated in arguments. Based on this model, we build a new corpus that comprises about 16k manual annotations of 4740 claims with instances of the model's elements, and we develop an end-to-end framework that automatically identifies all modeled types of instances. The results of experiments show the potential of the framework for building a web-based argumentation graph that is of high quality and large scale.{\textless}/p{\textgreater}},
author = {Al-Khatib, Khalid and Hou, Yufang and Wachsmuth, Henning and Jochim, Charles and Bonin, Francesca and Stein, Benno},
issn = {2374-3468},
journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
keywords = {Natural Language Processing},
number = {05},
pages = {7367--7374},
title = {{End-to-End Argumentation Knowledge Graph Construction}},
url = {https://aaai.org/ojs/index.php/AAAI/article/view/6231},
volume = {34},
year = {2020}
}
@article{Iv2019,
author = {Iv, Robert L Logan and Liu, Nelson F and Peters, Matthew E and Gardner, Matt},
journal = {Acl},
pages = {5962--5971},
title = {{Using Knowledge Graphs for Fact-Aware Language Modeling}},
year = {2019}
}
@article{Raph2020,
archivePrefix = {arXiv},
arxivId = {arXiv:2010.00796v1},
author = {Raph, G and Nderstanding, L Anguage U},
eprint = {arXiv:2010.00796v1},
pages = {1--11},
title = {{JAKET: JOINT PRE-TRAINING OF KNOWLEDGE GRAPH AND LANGUAGE UNDERSTANDING}},
year = {2020}
}
@article{Sun2020,
abstract = {With the emerging branch of incorporating factual knowledge into pre-trained language models such as BERT, most existing models consider shallow, static, and separately pre-trained entity embeddings, which limits the performance gains of these models. Few works explore the potential of deep contextualized knowledge representation when injecting knowledge. In this paper, we propose the Contextualized Language and Knowledge Embedding (CoLAKE), which jointly learns contextualized representation for both language and knowledge with the extended MLM objective. Instead of injecting only entity embeddings, CoLAKE extracts the knowledge context of an entity from large-scale knowledge bases. To handle the heterogeneity of knowledge context and language context, we integrate them in a unified data structure, word-knowledge graph (WK graph). CoLAKE is pre-trained on large-scale WK graphs with the modified Transformer encoder. We conduct experiments on knowledge-driven tasks, knowledge probing tasks, and language understanding tasks. Experimental results show that CoLAKE outperforms previous counterparts on most of the tasks. Besides, CoLAKE achieves surprisingly high performance on our synthetic task called word-knowledge graph completion, which shows the superiority of simultaneously contextualizing language and knowledge representation.},
archivePrefix = {arXiv},
arxivId = {2010.00309},
author = {Sun, Tianxiang and Shao, Yunfan and Qiu, Xipeng and Guo, Qipeng and Hu, Yaru and Huang, Xuanjing and Zhang, Zheng},
eprint = {2010.00309},
title = {{CoLAKE: Contextualized Language and Knowledge Embedding}},
url = {http://arxiv.org/abs/2010.00309},
year = {2020}
}
@article{Zhang2020,
abstract = {Commonsense knowledge acquisition is a key problem for artificial intelligence. Conventional methods of acquiring commonsense knowledge generally require laborious and costly human annotations, which are not feasible on a large scale. In this paper, we explore a practical way of mining commonsense knowledge from linguistic graphs, with the goal of transferring cheap knowledge obtained with linguistic patterns into expensive commonsense knowledge. The result is a conversion of ASER [Zhang et al., 2020], a large-scale selectional preference knowledge resource, into TransOMCS, of the same representation as ConceptNet [Liu and Singh, 2004] but two orders of magnitude larger. Experimental results demonstrate the transferability of linguistic knowledge to commonsense knowledge and the effectiveness of the proposed approach in terms of quantity, novelty, and quality. TransOMCS is publicly available at: https://github.com/HKUST-KnowComp/TransOMCS.},
archivePrefix = {arXiv},
arxivId = {2005.00206},
author = {Zhang, Hongming and Khashabi, Daniel and Song, Yangqiu and Roth, Dan},
doi = {10.24963/ijcai.2020/554},
eprint = {2005.00206},
pages = {4004--4010},
title = {{TransOMCS: From Linguistic Graphs to Commonsense Knowledge}},
year = {2020}
}
@article{Xiong2020,
author = {Xiong, Wenhan and Du, Jingfei and Wang, William Yang and Stoyanov, Veselin},
pages = {1--13},
title = {{Pretrained Encyclopedia : Weakly Supervised Knowledge-Pretrained Language Model}},
year = {2020}
}
@article{Sadeghian2019,
abstract = {In this paper, we study the problem of learning probabilistic logical rules for inductive and interpretable link prediction. Despite the importance of inductive link prediction, most previous works focused on transductive link prediction and cannot manage previously unseen entities. Moreover, they are black-box models that are not easily explainable for humans. We propose DRUM, a scalable and differentiable approach for mining first-order logical rules from knowledge graphs which resolves these problems. We motivate our method by making a connection between learning confidence scores for each rule and low-rank tensor approximation. DRUM uses bidirectional RNNs to share useful information across the tasks of learning rules for different relations. We also empirically demonstrate the efficiency of DRUM over existing rule mining methods for inductive link prediction on a variety of benchmark datasets.},
author = {Sadeghian, Ali and Armandpour, Mohammadreza and Ding, Patrick and Wang, Daisy Zhe},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
number = {NeurIPS},
pages = {1--11},
title = {{DRUM: End-to-end differentiable rule mining on knowledge graphs}},
volume = {32},
year = {2019}
}
@article{Anonymous2020,
archivePrefix = {arXiv},
arxivId = {arXiv:2009.11692v1},
author = {Anonymous},
eprint = {arXiv:2009.11692v1},
pages = {1--11},
title = {{Language Generation with Multi-hop Reasoning on Commonsense Knowledge Graph}},
year = {2020}
}
@article{Fei2020,
abstract = {We consider retrofitting structure-aware Transformer-based language model for facilitating end tasks by proposing to exploit syntactic distance to encode both the phrasal constituency and dependency connection into the language model. A middle-layer structural learning strategy is leveraged for structure integration, accomplished with main semantic task training under multi-task learning scheme. Experimental results show that the retrofitted structure-aware Transformer language model achieves improved perplexity, meanwhile inducing accurate syntactic phrases. By performing structure-aware fine-tuning, our model achieves significant improvements for both semantic- and syntactic-dependent tasks.},
archivePrefix = {arXiv},
arxivId = {2009.07408},
author = {Fei, Hao and Ren, Yafeng and Ji, Donghong},
eprint = {2009.07408},
title = {{Retrofitting Structure-aware Transformer Language Model for End Tasks}},
url = {http://arxiv.org/abs/2009.07408},
year = {2020}
}
@article{Lyu2020,
abstract = {We propose a suite of reasoning tasks on two types of relations between procedural events: goal-step relations ("learn poses" is a step in the larger goal of "doing yoga") and step-step temporal relations ("buy a yoga mat" typically precedes "learn poses"). We introduce a dataset targeting these two relations based on wikiHow, a website of instructional how-to articles. Our human-validated test set serves as a reliable benchmark for commonsense inference, with a gap of about 10{\%} to 20{\%} between the performance of state-of-the-art transformer models and human performance. Our automatically-generated training set allows models to effectively transfer to out-of-domain tasks requiring knowledge of procedural events, with greatly improved performances on SWAG, Snips, and the Story Cloze Test in zero- and few-shot settings.},
archivePrefix = {arXiv},
arxivId = {2009.07690},
author = {Lyu, Qing and Zhang, Li and Callison-Burch, Chris},
eprint = {2009.07690},
title = {{Reasoning about Goals, Steps, and Temporal Ordering with WikiHow}},
url = {http://arxiv.org/abs/2009.07690},
year = {2020}
}
@article{Han2020,
abstract = {Extracting event temporal relations is a critical task for information extraction and plays an important role in natural language understanding. Prior systems leverage deep learning and pre-trained language models to improve the performance of the task. However, these systems often suffer from two short-comings: 1) when performing maximum a posteriori (MAP) inference based on neural models, previous systems only used structured knowledge that are assumed to be absolutely correct, i.e., hard constraints; 2) biased predictions on dominant temporal relations when training with a limited amount of data. To address these issues, we propose a framework that enhances deep neural network with distributional constraints constructed by probabilistic domain knowledge. We solve the constrained inference problem via Lagrangian Relaxation and apply it on end-to-end event temporal relation extraction tasks. Experimental results show our framework is able to improve the baseline neural network models with strong statistical significance on two widely used datasets in news and clinical domains.},
archivePrefix = {arXiv},
arxivId = {2009.07373},
author = {Han, Rujun and Zhou, Yichao and Peng, Nanyun},
eprint = {2009.07373},
title = {{Domain Knowledge Empowered Structured Neural Net for End-to-End Event Temporal Relation Extraction}},
url = {http://arxiv.org/abs/2009.07373},
year = {2020}
}
@article{Malaviya2020,
abstract = {Automatic KB completion for commonsense knowledge graphs (e.g., ATOMIC and ConceptNet) poses unique challenges compared to the much studied conventional knowledge bases (e.g., Freebase). Commonsense knowledge graphs use free-form text to represent nodes, resulting in orders of magnitude more nodes compared to conventional KBs ( ∼18x more nodes in ATOMIC compared to Freebase (FB15K-237)). Importantly, this implies significantly sparser graph structures — a major challenge for existing KB completion methods that assume densely connected graphs over a relatively smaller set of nodes.In this paper, we present novel KB completion models that can address these challenges by exploiting the structural and semantic context of nodes. Specifically, we investigate two key ideas: (1) learning from local graph structure, using graph convolutional networks and automatic graph densification and (2) transfer learning from pre-trained language models to knowledge graphs for enhanced contextual representation of knowledge. We describe our method to incorporate information from both these sources in a joint model and provide the first empirical results for KB completion on ATOMIC and evaluation with ranking metrics on ConceptNet. Our results demonstrate the effectiveness of language model representations in boosting link prediction performance and the advantages of learning from local graph structure (+1.5 points in MRR for ConceptNet) when training on subgraphs for computational efficiency. Further analysis on model predictions shines light on the types of commonsense knowledge that language models capture well.},
archivePrefix = {arXiv},
arxivId = {1910.02915},
author = {Malaviya, Chaitanya and Bhagavatula, Chandra and Bosselut, Antoine and Choi, Yejin},
doi = {10.1609/aaai.v34i03.5684},
eprint = {1910.02915},
issn = {2159-5399},
journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
number = {03},
pages = {2925--2933},
title = {{Commonsense Knowledge Base Completion with Structural and Semantic Context}},
volume = {34},
year = {2020}
}
@article{Zhou2020,
abstract = {Temporal common sense (e.g., duration and frequency of events) is crucial for understanding natural language. However, its acquisition is challenging, partly because such information is often not expressed explicitly in text, and human annotation on such concepts is costly. This work proposes a novel sequence modeling approach that exploits explicit and implicit mentions of temporal common sense, extracted from a large corpus, to build TACOLM, a temporal common sense language model. Our method is shown to give quality predictions of various dimensions of temporal common sense (on UDST and a newly collected dataset from RealNews). It also produces representations of events for relevant tasks such as duration comparison, parent-child relations, event coreference and temporal QA (on TimeBank, HiEVE and MCTACO) that are better than using the standard BERT. Thus, it will be an important component of temporal NLP.},
archivePrefix = {arXiv},
arxivId = {2005.04304},
author = {Zhou, Ben and Ning, Qiang and Khashabi, Daniel and Roth, Dan},
doi = {10.18653/v1/2020.acl-main.678},
eprint = {2005.04304},
pages = {7579--7589},
title = {{Temporal Common Sense Acquisition with Minimal Supervision}},
year = {2020}
}
@article{Romero2019,
abstract = {Commonsense knowledge about object properties, human behavior and general concepts is crucial for robust AI applications. However, automatic acquisition of this knowledge is challenging because of sparseness and bias in online sources. This paper presents Quasimodo, a methodology and tool suite for distilling commonsense properties from non-standard web sources. We devise novel ways of tapping into search-engine query logs and QA forums, and combining the resulting candidate assertions with statistical cues from encyclopedias, books and image tags in a corroboration step. Unlike prior work on commonsense knowledge bases, Quasimodo focuses on salient properties that are typically associated with certain objects or concepts. Extensive evaluations, including extrinsic use-case studies, show that Quasimodo provides better coverage than state-of-the-art baselines with comparable quality.},
archivePrefix = {arXiv},
arxivId = {1905.10989},
author = {Romero, Julien and Pan, Jeff Z. and Razniewski, Simon and Sakhadeo, Archit and Pal, Koninika and Weikum, Gerhard},
doi = {10.1145/3357384.3357955},
eprint = {1905.10989},
isbn = {9781450369763},
journal = {International Conference on Information and Knowledge Management, Proceedings},
pages = {1411--1420},
title = {{Commonsense properties from query logs and question answering forums}},
year = {2019}
}
@techreport{Zhu2019,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Zhu, Kenny Q},
booktitle = {Journal of Chemical Information and Modeling},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
isbn = {9788578110796},
issn = {1098-6596},
keywords = {icle},
number = {9},
pages = {1689--1699},
pmid = {25246403},
title = {{NSFC(Commonsense)}},
volume = {53},
year = {2019}
}
@article{Storks2019,
abstract = {In the NLP community, recent years have seen a surge of research activities that address machines' ability to perform deep language understanding which goes beyond what is explicitly stated in text, rather relying on reasoning and knowledge of the world. Many benchmark tasks and datasets have been created to support the development and evaluation of such natural language inference ability. As these benchmarks become instrumental and a driving force for the NLP research community, this paper aims to provide an overview of recent benchmarks, relevant knowledge resources, and state-of-the-art learning and inference approaches in order to support a better understanding of this growing field.},
archivePrefix = {arXiv},
arxivId = {1904.01172},
author = {Storks, Shane and Gao, Qiaozi and Chai, Joyce Y.},
eprint = {1904.01172},
pages = {1--60},
title = {{Recent Advances in Natural Language Inference: A Survey of Benchmarks, Resources, and Approaches}},
url = {http://arxiv.org/abs/1904.01172},
year = {2019}
}
@article{Roth2020,
author = {Roth, Dan},
number = {July},
title = {{Temporal Commonsense}},
year = {2020}
}
@article{Klein2020,
abstract = {We propose a self-supervised method to solve Pronoun Disambiguation and Winograd Schema Challenge problems. Our approach exploits the characteristic structure of training corpora related to so-called "trigger" words, which are responsible for flipping the answer in pronoun disambiguation. We achieve such commonsense reasoning by constructing pair-wise contrastive auxiliary predictions. To this end, we leverage a mutual exclusive loss regularized by a contrastive margin. Our architecture is based on the recently introduced transformer networks, BERT, that exhibits strong performance on many NLP benchmarks. Empirical results show that our method alleviates the limitation of current supervised approaches for commonsense reasoning. This study opens up avenues for exploiting inexpensive self-supervision to achieve performance gain in commonsense reasoning tasks.},
archivePrefix = {arXiv},
arxivId = {2005.00669},
author = {Klein, Tassilo and Nabi, Moin},
doi = {10.18653/v1/2020.acl-main.671},
eprint = {2005.00669},
pages = {7517--7523},
title = {{Contrastive Self-Supervised Learning for Commonsense Reasoning}},
year = {2020}
}
@article{Sap2020,
author = {Sap, Maarten and Shwartz, Vered and Bosselut, Antoine and Choi, Yejin and Roth, Dan},
doi = {10.18653/v1/2020.acl-tutorials.7},
pages = {27--33},
title = {{Commonsense Reasoning for Natural Language Processing}},
year = {2020}
}
@article{Shwartz2020,
abstract = {ACL 2020 Commonsense Tutorial (T6) Commonsense knowledge, such as knowing that "bumping into people annoys them" or "rain makes the road slippery", helps humans navigate everyday situations seamlessly. Yet, endowing machines with such human-like commonsense reasoning capabilities has remained an elusive goal of artificial intelligence research for decades. This tutorial will discuss various challenges related to commonsense reasoning for AI, including how to represent and measure it, as well as incorporate it into downstream tasks.},
author = {Shwartz, Vered},
title = {{Commonsense Knowledge in Pre-trained Language Models}},
url = {https://homes.cs.washington.edu/{~}msap/acl2020-commonsense/},
year = {2020}
}
@article{Talmor2019,
abstract = {When answering a question, people often draw upon their rich world knowledge in addition to the particular context. Recent work has focused primarily on answering questions given some relevant document or context, and required very little general background. To investigate question answering with prior knowledge, we present COMMONSENSEQA: a challenging new dataset for commonsense question answering. To capture common sense beyond associations, we extract from CONCEPTNET (Speer et al., 2017) multiple target concepts that have the same semantic relation to a single source concept. Crowd-workers are asked to author multiple-choice questions that mention the source concept and discriminate in turn between each of the target concepts. This encourages workers to create questions with complex semantics that often require prior knowledge. We create 12,247 questions through this procedure and demonstrate the difficulty of our task with a large number of strong baselines. Our best baseline is based on BERT-large (Devlin et al., 2018) and obtains 56{\%} accuracy, well below human performance, which is 89{\%}.},
author = {Talmor, Alon and Herzig, Jonathan and Lourie, Nicholas and Berant, Jonathan},
isbn = {9781950737130},
journal = {NAACL HLT 2019 - 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies - Proceedings of the Conference},
pages = {4149--4158},
title = {{CommonSenseqa: A question answering challenge targeting commonsense knowledge}},
volume = {1},
year = {2019}
}
@article{Sap2019,
abstract = {We present ATOMIC, an atlas of everyday commonsense reasoning, organized through 877k textual descriptions of inferential knowledge. Compared to existing resources that center around taxonomic knowledge, ATOMIC focuses on inferential knowledge organized as typed if-then relations with variables (e.g., “if X pays Y a compliment, then Y will likely return the compliment”). We propose nine if-then relation types to distinguish causes vs. effects, agents vs. themes, voluntary vs. involuntary events, and actions vs. mental states. By generatively training on the rich inferential knowledge described in ATOMIC, we show that neural models can acquire simple commonsense capabilities and reason about previously unseen events. Experimental results demonstrate that multitask models that incorporate the hierarchical structure of if-then relation types lead to more accurate inference compared to models trained in isolation, as measured by both automatic and human evaluation.},
archivePrefix = {arXiv},
arxivId = {1811.00146},
author = {Sap, Maarten and {Le Bras}, Ronan and Allaway, Emily and Bhagavatula, Chandra and Lourie, Nicholas and Rashkin, Hannah and Roof, Brendan and Smith, Noah A. and Choi, Yejin},
doi = {10.1609/aaai.v33i01.33013027},
eprint = {1811.00146},
issn = {2159-5399},
journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
keywords = {Description Logics, Tractable Reasoning, OBDA},
pages = {3027--3035},
title = {{ATOMIC: An Atlas of Machine Commonsense for If-Then Reasoning}},
volume = {33},
year = {2019}
}
@article{,
annote = {1. OpenCyc, NELL: complext logical form representation
2. ConceptNet, ATOMIC: commonsense knowledge in natural language form collected through crowd-sourcing or game with a purpose.

ConceptNet 5.x:
1. commonsense knowledge about general objects or concpets.},
title = {{Commonsense resources}}
}
@article{,
title = {{Commonsense benchmarks}}
}
@article{Wu2020,
abstract = {Generative dialogue systems tend to produce generic responses, which often leads to boring conversations. For alleviating this issue, Recent studies proposed to retrieve and introduce knowledge facts from knowledge graphs. While this paradigm works to a certain extent , it usually retrieves knowledge facts only based on the entity word itself, without considering the specific dialogue context. Thus, the introduction of the context-irrelevant knowledge facts can impact the quality of generations. To this end, this paper proposes a novel commonsense knowledge-aware dialogue generation model, ConKADI. We design a Felicitous Fact mechanism to help the model focus on the knowledge facts that are highly relevant to the context; furthermore, two techniques , Context-Knowledge Fusion and Flexible Mode Fusion are proposed to facilitate the integration of the knowledge in the ConKADI. We collect and build a large-scale Chinese dataset aligned with the commonsense knowledge for dialogue generation. Extensive evaluations over both an open-released English dataset and our Chinese dataset demonstrate that our approach ConKADI outperforms the state-of-the-art approach CCM, in most experiments .},
author = {Wu, Sixing and Li, Ying and Zhang, Dawei and Zhou, Yang and Wu, Zhonghai},
doi = {10.18653/v1/2020.acl-main.515},
pages = {5811--5820},
title = {{Diverse and Informative Dialogue Generation with Context-Specific Commonsense Knowledge Awareness}},
year = {2020}
}
@article{Hwang2020,
abstract = {Recent years have brought about a renewed interest in commonsense representation and reasoning in the field of natural language understanding. The development of new commonsense knowledge graphs (CSKG) has been central to these advances as their diverse facts can be used and referenced by machine learning models for tackling new and challenging tasks. At the same time, there remain questions about the quality and coverage of these resources due to the massive scale required to comprehensively encompass general commonsense knowledge. In this work, we posit that manually constructed CSKGs will never achieve the coverage necessary to be applicable in all situations encountered by NLP agents. Therefore, we propose a new evaluation framework for testing the utility of KGs based on how effectively implicit knowledge representations can be learned from them. With this new goal, we propose ATOMIC 2020, a new CSKG of general-purpose commonsense knowledge containing knowledge that is not readily available in pretrained language models. We evaluate its properties in comparison with other leading CSKGs, performing the first large-scale pairwise study of commonsense knowledge resources. Next, we show that ATOMIC 2020 is better suited for training knowledge models that can generate accurate, representative knowledge for new, unseen entities and events. Finally, through human evaluation, we show that the few-shot performance of GPT-3 (175B parameters), while impressive, remains {\~{}}12 absolute points lower than a BART-based knowledge model trained on ATOMIC 2020 despite using over 430x fewer parameters.},
archivePrefix = {arXiv},
arxivId = {2010.05953},
author = {Hwang, Jena D. and Bhagavatula, Chandra and Bras, Ronan Le and Da, Jeff and Sakaguchi, Keisuke and Bosselut, Antoine and Choi, Yejin},
eprint = {2010.05953},
title = {{COMET-ATOMIC 2020: On Symbolic and Neural Commonsense Knowledge Graphs}},
url = {http://arxiv.org/abs/2010.05953},
year = {2020}
}
@article{Bosselut2019,
abstract = {We present the first comprehensive study on automatic knowledge base construction for two prevalent commonsense knowledge graphs: ATOMIC (Sap et al., 2019) and ConceptNet (Speer et al., 2017). Contrary to many conventional KBs that store knowledge with canonical templates, commonsense KBs only store loosely structured open-text descriptions of knowledge. We posit that an important step toward automatic commonsense completion is the development of generative models of commonsense knowledge, and propose COMmonsEnse Transformers (COMET) that learn to generate rich and diverse commonsense descriptions in natural language. Despite the challenges of commonsense modeling, our investigation reveals promising results when implicit knowledge from deep pre-trained language models is transferred to generate explicit knowledge in commonsense knowledge graphs. Empirical results demonstrate that COMET is able to generate novel knowledge that humans rate as high quality, with up to 77.5{\%} (ATOMIC) and 91.7{\%} (ConceptNet) precision at top 1, which approaches human performance for these resources. Our findings suggest that using generative commonsense models for automatic commonsense KB completion could soon be a plausible alternative to extractive methods.},
archivePrefix = {arXiv},
arxivId = {1906.05317},
author = {Bosselut, Antoine and Rashkin, Hannah and Sap, Maarten and Malaviya, Chaitanya and Celikyilmaz, Asli and Choi, Yejin},
doi = {10.18653/v1/p19-1470},
eprint = {1906.05317},
pages = {4762--4779},
title = {{COMET: Commonsense Transformers for Automatic Knowledge Graph Construction}},
year = {2019}
}
@article{Rogers2020,
abstract = {Transformer-based models are now widely used in NLP, but we still do not understand a lot about their inner workings. This paper describes what is known to date about the famous BERT model (Devlin et al. 2019), synthesizing over 40 analysis studies. We also provide an overview of the proposed modifications to the model and its training regime. We then outline the directions for further research.},
archivePrefix = {arXiv},
arxivId = {2002.12327},
author = {Rogers, Anna and Kovaleva, Olga and Rumshisky, Anna},
eprint = {2002.12327},
title = {{A Primer in BERTology: What we know about how BERT works}},
url = {http://arxiv.org/abs/2002.12327},
year = {2020}
}
@article{Vulic2020,
abstract = {The success of large pretrained language models (LMs) such as BERT and RoBERTa has sparked interest in probing their representations, in order to unveil what types of knowledge they implicitly capture. While prior research focused on morphosyntactic, semantic, and world knowledge, it remains unclear to which extent LMs also derive lexical type-level knowledge from words in context. In this work, we present a systematic empirical analysis across six typologically diverse languages and five different lexical tasks, addressing the following questions: 1) How do different lexical knowledge extraction strategies (monolingual versus multilingual source LM, out-of-context versus in-context encoding, inclusion of special tokens, and layer-wise averaging) impact performance? How consistent are the observed effects across tasks and languages? 2) Is lexical knowledge stored in few parameters, or is it scattered throughout the network? 3) How do these representations fare against traditional static word vectors in lexical tasks? 4) Does the lexical information emerging from independently trained monolingual LMs display latent similarities? Our main results indicate patterns and best practices that hold universally, but also point to prominent variations across languages and tasks. Moreover, we validate the claim that lower Transformer layers carry more type-level lexical knowledge, but also show that this knowledge is distributed across multiple layers.},
archivePrefix = {arXiv},
arxivId = {2010.05731},
author = {Vuli{\'{c}}, Ivan and Ponti, Edoardo Maria and Litschko, Robert and Glava{\v{s}}, Goran and Korhonen, Anna},
eprint = {2010.05731},
title = {{Probing Pretrained Language Models for Lexical Semantics}},
url = {http://arxiv.org/abs/2010.05731},
year = {2020}
}
@article{Zhang2020a,
abstract = {Pretrained Language Models (LMs) have been shown to possess significant linguistic, common sense, and factual knowledge. One form of knowledge that has not been studied yet in this context is information about the scalar magnitudes of objects. We show that pretrained language models capture a significant amount of this information but are short of the capability required for general common-sense reasoning. We identify contextual information in pre-training and numeracy as two key factors affecting their performance and show that a simple method of canonicalizing numbers can have a significant effect on the results.},
archivePrefix = {arXiv},
arxivId = {2010.05345},
author = {Zhang, Xikun and Ramachandran, Deepak and Tenney, Ian and Elazar, Yanai and Roth, Dan},
eprint = {2010.05345},
title = {{Do Language Embeddings Capture Scales?}},
url = {http://arxiv.org/abs/2010.05345},
year = {2020}
}
@article{Talmor2020,
abstract = {To what extent can a neural network systematically reason over symbolic facts? Evidence suggests that large pre-trained language models (LMs) acquire some reasoning capacity, but this ability is difficult to control. Recently, it has been shown that Transformer-based models succeed in consistent reasoning over explicit symbolic facts, under a "closed-world" assumption. However, in an open-domain setup, it is desirable to tap into the vast reservoir of implicit knowledge already encoded in the parameters of pre-trained LMs. In this work, we provide a first demonstration that LMs can be trained to reliably perform systematic reasoning combining both implicit, pre-trained knowledge and explicit natural language statements. To do this, we describe a procedure for automatically generating datasets that teach a model new reasoning skills, and demonstrate that models learn to effectively perform inference which involves implicit taxonomic and world knowledge, chaining and counting. Finally, we show that "teaching" models to reason generalizes beyond the training distribution: they successfully compose the usage of multiple reasoning skills in single examples. Our work paves a path towards open-domain systems that constantly improve by interacting with users who can instantly correct a model by adding simple natural language statements.},
archivePrefix = {arXiv},
arxivId = {2006.06609},
author = {Talmor, Alon and Tafjord, Oyvind and Clark, Peter and Goldberg, Yoav and Berant, Jonathan},
eprint = {2006.06609},
pages = {1--13},
title = {{Teaching Pre-Trained Models to Systematically Reason Over Implicit Knowledge}},
url = {http://arxiv.org/abs/2006.06609},
year = {2020}
}
@article{Pereira2020,
abstract = {We propose an AdversariaL training algorithm for commonsense InferenCE (ALICE). We apply small perturbations to word embeddings and minimize the resultant adversarial risk to regularize the model. We exploit a novel combination of two different approaches to estimate these perturbations: 1) using the true label and 2) using the model prediction. Without relying on any human-crafted features, knowledge bases, or additional datasets other than the target datasets, our model boosts the fine-tuning performance of RoBERTa, achieving competitive results on multiple reading comprehension datasets that require commonsense inference.},
archivePrefix = {arXiv},
arxivId = {2005.08156},
author = {Pereira, Lis and Liu, Xiaodong and Cheng, Fei and Asahara, Masayuki and Kobayashi, Ichiro},
doi = {10.18653/v1/2020.repl4nlp-1.8},
eprint = {2005.08156},
pages = {55--60},
title = {{Adversarial Training for Commonsense Inference}},
year = {2020}
}
@article{Elazar2019,
archivePrefix = {arXiv},
arxivId = {arXiv:1912.13283v1},
author = {Elazar, Yanai and Goldberg, Yoav},
eprint = {arXiv:1912.13283v1},
title = {{oLMpics - On what Language Model Pre-training Captures}},
year = {2019}
}
@article{Petroni2020,
abstract = {Recent progress in pretraining language models on large textual corpora led to a surge of improvements for downstream NLP tasks. Whilst learning linguistic knowledge, these models may also be storing relational knowledge present in the training data, and may be able to answer queries structured as “fill-in-the-blank” cloze statements. Language models have many advantages over structured knowledge bases: they require no schema engineering, allow practitioners to query about an open class of relations, are easy to extend to more data, and require no human supervision to train. We present an in-depth analysis of the relational knowledge already present (without fine-tuning) in a wide range of state-of-the-art pretrained language models. We find that (i) without fine-tuning, BERT contains relational knowledge competitive with traditional NLP methods that have some access to oracle knowledge, (ii) BERT also does remarkably well on open-domain question answering against a supervised baseline, and (iii) certain types of factual knowledge are learned much more readily than others by standard language model pretraining approaches. The surprisingly strong ability of these models to recall factual knowledge without any fine-tuning demonstrates their potential as unsupervised open-domain QA systems. The code to reproduce our analysis is available at https://github.com/facebookresearch/LAMA.},
archivePrefix = {arXiv},
arxivId = {1909.01066},
author = {Petroni, Fabio and Rockt{\"{a}}schel, Tim and Lewis, Patrick and Bakhtin, Anton and Wu, Yuxiang and Miller, Alexander H. and Riedel, Sebastian},
doi = {10.18653/v1/d19-1250},
eprint = {1909.01066},
isbn = {9781950737901},
journal = {EMNLP-IJCNLP 2019 - 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, Proceedings of the Conference},
pages = {2463--2473},
title = {{Language models as knowledge bases?}},
year = {2020}
}
@article{Zhang2020b,
annote = {eventuality-centered knowledge graph about real-world commonsense knowledge.
type of nodes:
1. activity
2. state
3. event

Differnence with ATOMIC:
1. ATOMIC only contains cause-effect, such inferential/causal commonsense knowledge between events.
2. ASER contains 15 different relation types between two eventualities.},
archivePrefix = {arXiv},
arxivId = {arXiv:1905.00270v3},
author = {Zhang, Hongming},
eprint = {arXiv:1905.00270v3},
isbn = {9781450370233},
title = {{ASER : A Large-scale Eventuality Knowledge Graph}},
volume = {2},
year = {2020}
}
@article{Liu,
archivePrefix = {arXiv},
arxivId = {arXiv:2010.04043v1},
author = {Liu, Haokun and Huang, William and Mungra, Dhara A and Bowman, Samuel R},
eprint = {arXiv:2010.04043v1},
title = {{Precise Task Formalization Matters in Winograd Schema Evaluations}}
}
@article{Lewis2020,
abstract = {Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.},
archivePrefix = {arXiv},
arxivId = {2005.11401},
author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandara and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"{u}}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"{a}}schel, Tim and Riedel, Sebastian and Kiela, Douwe},
eprint = {2005.11401},
title = {{Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks}},
url = {http://arxiv.org/abs/2005.11401},
year = {2020}
}
@article{Gabriel2020,
archivePrefix = {arXiv},
arxivId = {arXiv:2010.01486v1},
author = {Gabriel, Saadia and Bhagavatula, Chandra and Shwartz, Vered and Le, Ronan and Forbes, Maxwell and Choi, Yejin},
eprint = {arXiv:2010.01486v1},
title = {{Paragraph-level Commonsense Transformers with Recurrent Memory}},
year = {2020}
}
@article{Bouraoui2019,
abstract = {One of the most remarkable properties of word embeddings is the fact that they capture certain types of semantic and syntactic relationships. Recently, pre-trained language models such as BERT have achieved groundbreaking results across a wide range of Natural Language Processing tasks. However, it is unclear to what extent such models capture relational knowledge beyond what is already captured by standard word embeddings. To explore this question, we propose a methodology for distilling relational knowledge from a pre-trained language model. Starting from a few seed instances of a given relation, we first use a large text corpus to find sentences that are likely to express this relation. We then use a subset of these extracted sentences as templates. Finally, we fine-tune a language model to predict whether a given word pair is likely to be an instance of some relation, when given an instantiated template for that relation as input.},
archivePrefix = {arXiv},
arxivId = {1911.12753},
author = {Bouraoui, Zied and Camacho-Collados, Jose and Schockaert, Steven},
doi = {10.1609/aaai.v34i05.6242},
eprint = {1911.12753},
issn = {2374-3468},
title = {{Inducing Relational Knowledge from BERT}},
url = {http://arxiv.org/abs/1911.12753},
year = {2019}
}
@article{Hobbs2020,
abstract = {Abductive reasoning is inference to the most plausible explanation. For example, if Jenny finds her house in a mess when she returns from work, and remembers that she left a window open, she can hypothesize that a thief broke into her house and caused the mess, as the most plausible explanation. While abduction has long been considered to be at the core of how people interpret and read between the lines in natural language (Hobbs et al., 1988), there has been relatively little research in support of abductive natural language inference and generation. We present the first study that investigates the viability of language-based abductive reasoning. We introduce a challenge dataset, ART, that consists of over 20k commonsense narrative contexts and 200k explanations. Based on this dataset, we conceptualize two new tasks – (i) Abductive NLI: a multiple-choice question answering task for choosing the more likely explanation, and (ii) Abductive NLG: a conditional generation task for explaining given observations in natural language. On Abductive NLI, the best model achieves 68.9{\%} accuracy, well below human performance of 91.4{\%}. On Abductive NLG, the current best language generators struggle even more, as they lack reasoning capabilities that are trivial for humans. Our analysis leads to new insights into the types of reasoning that deep pre-trained language models fail to perform—despite their strong performance on the related but more narrowly defined task of entailment NLI—pointing to interesting avenues for future research.},
archivePrefix = {arXiv},
arxivId = {arXiv:submit/3044704},
author = {Hobbs, Jerry R.},
doi = {10.1162/COLI_a_00171},
eprint = {3044704},
issn = {15309312},
journal = {Computational Linguistics},
number = {4},
pages = {781--798},
primaryClass = {arXiv:submit},
title = {{ABDUCTIVE COMMONSENSE REASONING}},
volume = {39},
year = {2020}
}
@article{chen2017neural,
  title={Neural natural language inference models enhanced with external knowledge},
  author={Chen, Qian and Zhu, Xiaodan and Ling, Zhen-Hua and Inkpen, Diana and Wei, Si},
  journal={arXiv preprint arXiv:1711.04289},
  year={2017}
}
@inproceedings{yang2019enhancing,
  title={Enhancing pre-trained language representations with rich knowledge for machine reading comprehension},
  author={Yang, An and Wang, Quan and Liu, Jing and Liu, Kai and Lyu, Yajuan and Wu, Hua and She, Qiaoqiao and Li, Sujian},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={2346--2357},
  year={2019}
}
@inproceedings{wu2020diverse,
  title={Diverse and Informative Dialogue Generation with Context-Specific Commonsense Knowledge Awareness},
  author={Wu, Sixing and Li, Ying and Zhang, Dawei and Zhou, Yang and Wu, Zhonghai},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={5811--5820},
  year={2020}
}

@inproceedings{davison2019commonsense,
  title={Commonsense knowledge mining from pretrained models},
  author={Davison, Joe and Feldman, Joshua and Rush, Alexander M},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages={1173--1178},
  year={2019}
}

@article{bosselut2019dynamic,
  title={Dynamic knowledge graph construction for zero-shot commonsense question answering},
  author={Bosselut, Antoine and Choi, Yejin},
  journal={arXiv preprint arXiv:1911.03876},
  year={2019}
}

@article{shwartz2020unsupervised,
  title={Unsupervised Commonsense Question Answering with Self-Talk},
  author={Shwartz, Vered and West, Peter and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
  journal={arXiv preprint arXiv:2004.05483},
  year={2020}
}

